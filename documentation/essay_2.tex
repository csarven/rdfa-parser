%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[hidelinks, a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images

\usepackage{hyperref} % use links for ToC, etc.


%Define the listing package for JavaScript
\usepackage{listings} %code highlighter
\usepackage{color} %use color
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
 
%Customize listing look (code snippets)
\lstset{ %
backgroundcolor=\color{white}, % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
basicstyle=\footnotesize, % the size of the fonts that are used for the code
breakatwhitespace=false, % sets if automatic breaks should only happen at whitespace
breaklines=true, % sets automatic line breaking
captionpos=b, % sets the caption-position to bottom
commentstyle=\color{mygreen}, % comment style
deletekeywords={...}, % if you want to delete keywords from the given language
escapeinside={\%*}{*)}, % if you want to add LaTeX within your code
extendedchars=true, % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
frame=single, % adds a frame around the code
keepspaces=true, % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
keywordstyle=\color{blue}, % keyword style
% language=Octave, % the language of the code
morekeywords={*,...}, % if you want to add more keywords to the set
numbers=left, % where to put the line-numbers; possible values are (none, left, right)
numbersep=5pt, % how far the line-numbers are from the code
numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
rulecolor=\color{black}, % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
showspaces=false, % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
showstringspaces=false, % underline spaces within strings only
showtabs=false, % show tabs within strings adding particular underscores
stepnumber=1, % the step between two line-numbers. If it's 1, each line will be numbered
stringstyle=\color{mymauve}, % string literal style
tabsize=2, % sets default tabsize to 2 spaces
title=\lstname % show the filename of files included with \lstinputlisting; also try caption instead of title
}
%END of listing package%
 
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
 
%define Javascript language
\lstdefinelanguage{JavaScript}{
keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break, let, for},
keywordstyle=\color{blue}\bfseries,
ndkeywords={class, export, boolean, throw, implements, import, this},
ndkeywordstyle=\color{darkgray}\bfseries,
identifierstyle=\color{black},
sensitive=false,
comment=[l]{//},
morecomment=[s]{/*}{*/},
commentstyle=\color{purple}\ttfamily,
stringstyle=\color{red}\ttfamily,
morestring=[b]',
morestring=[b]"
}
 
\lstset{
language=JavaScript,
extendedchars=true,
basicstyle=\footnotesize\ttfamily,
showstringspaces=false,
showspaces=false,
numbers=left,
numberstyle=\footnotesize,
numbersep=9pt,
tabsize=2,
breaklines=true,
showtabs=false,
captionpos=b
}

\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}


%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{RDFa Crawler}\\ % Title
Webcrawler inklusive RDFa parsing funktion} % Subtitle

\author{\textsc{Stefan Achm\"uller, Roland Gritzer, Mathias Gschwandtner} % Author
\\{\textit{Universit\"at Innsbruck}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle % Print the title section


%----------------------------------------------------------------------------------------
%	ABSTRACT, KEYWORDS AND TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\renewcommand{\abstractname}{Zusammenfassung} % Uncomment to change the name of the abstract to something else

\begin{abstract}
Ein auf JavaScript basierte Bibliothek zur Durchsuchung von RDFa annotierten Informationen, welche \"uber Internetzugriff einsehbar sind. Ausgehend von einer URL, mit optional m\"oglichem Ver- und Gebietens von URL Dom\"anen, werden zusammenh\"angende XHTML Dokumente nach RDF Trippel durchsucht.
\end{abstract}

\hspace*{3,6mm}\textit{Schl\"usselw\"orter:} webcrawler, parser, rdfa, node, npm % Keywords

\vspace{30pt} % Some vertical space between the abstract and first section

\renewcommand{\contentsname}{Inhaltsangabe}

\tableofcontents
\newpage

% GRADING ASPECT
% Result of written Report:
% 	2-6 pages
% 	Problem definition (what and why is it relevant?)
% 	Methodology (how did we solve?)
% 	Outcome (result of our work incl. Readme.md)

% Inline image example
% \begin{wrapfigure}{l}{0.4\textwidth} 
% \begin{center}
% \includegraphics[width=0.38\textwidth]{fish.png}
% \end{center}
% \caption{Fish}
% \end{wrapfigure}


%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section{Problem Definition}

Mit der historischen Entwicklung des Internets wurden immer wieder Techniken eingef\"uhrt, die den Datenaustausch zwischen Rechnern vereinfachen. W\"ahrend im klassischen Internet, auch Web 1.0 (URL, HTTP, HTML, etc.) genannt, der Fokus auf dem Aufbau und Transport der Daten liegt, betrachtet der Ansatz "Semantic Web" m\"ogliche Interpretationen der Daten. Dies wird auch Web 3.0 genannt und erm\"oglicht eine vereinfachte Abarbeitung von Aufgaben, basierend auf Internetdaten. So kann zum Beispiel unterschieden werden, ob das Wort "Bremen" auf einer bestimmten Webseite sich entweder auf eine deutsche Stadt, einen Familiennamen oder einem sonstigen Namen bezieht. Die Kerneigenschaft des "Semantic Web" stellt die Universalit\"at der Relationten, sowie die maschinelle Interpretations\"omglichkeit der Informationen dar. Dies bedeutet, dass prinzipiell alle Informationsobjekte miteinander verkn\"upft werden k\"onnen um Wissen zu repr\"asentieren und verarbeiten \cite{berners2001semantic}. \\

Um Daten mit diesen Metainformationen anzureichern, wurden diverse Annotationen eingef\"uhrt. Neben JSON-LD und Microdata, bietet sich hierf\"ur das "Resource Description Frameworks" (RDF) an. Ziel dieser Arbeit ist die Erstellung einer Programmierbibliothek f\"ur den JavaScript basierten "Node Package Manager" (npm) zum automatischen extrahieren von kontextspezifischen Informationen, Schema.org Vokabular annotiert mittels RDFa Syntax (W3C Standard Annotation f\"ur RDF), welche in XHTML Webseiten eingebettet sind \cite{guha2016schema}, \cite{halb2008building}.

\subsection{RDFa annotierte Information}

RDF definiert eine Schnittstelle f\"ur diverse Annotationen zur Darstellung von semantischen Informationen. Zu diesem geh\"oren die schematische Darstellung von Informationen als Graphen. Ein Graph ist hierbei eine Menge von Tripeln, welche einzelne Informationen repr\"asentieren. Ein Tripel besteht jeweils aus drei verschiedenen Knoten, die in folgender Reihe definiert sind. 

\begin{itemize}
\item \textbf{Subjekt} der Information (z.B. "Hannes ...").
\item \textbf{Pr\"adikat} der Information (z.B. "... wohnt in ...").
\item \textbf{Objekt} der Information (z.B. "... Innsbruck").
\end{itemize}

Jeder Knoten ist entweder ein "International Resource Identifier" (IRI), oder ein "Blank Node", welcher typischerweise als einmalige Zeichenketten (String) dargestellt wird \cite{adida2008rdfa}. Teilziel der Arbeit ist die Filtern von RDFa annotierten Information von Dokumenten im XHTML Format. Der implementierte Prozess der Erstellung eines RDF Graphen basiert auf der "RDFa Core Sequence" \cite{rdfaSequence}.

\subsection{Webcrawler}

Der zweite Teil der Arbeit, bezieht sich auf den Bezug von XHTML formatierten Dokumenten, welche RDFa annotierte Information enthalten. Mittels der Methode eines Webcrawlers wird auf im Internet zugreifbare Daten automatisch zugegriffen. Dabei definiert der Benutzer einen Startpunkt, ein "Uniform Resource Locater" (URL), auf den dieser Zugriff hat. Ausgehend von dieser Ressource, typischerweise ein XHTML Dokument, werden weiterf\"uhrende URLs durchsucht. Wie diese Durchsuchung im Einzelnen abl\"auft, zum Beispiel welche Datenformate nicht durchsucht werden sollten, wird vom Benutzer festgelegt \cite{pinkerton2000webcrawler}. 
Teilziel der Arbeit ist der Zugriff auf zusammenh\"angende XHTML formatierte Daten, ausgehend von einer festgelegten URL.


%------------------------------------------------

\section{Methodologie}

Nach der Definition der Aufgabenstellung, wurden die Schritte zur Probleml\"osung, inklusive einem groben Zeitplan, erstellt. Zum einen dient diese Strukturierung zur leichteren Abgrenzung von Teilproblemen und deren Bearbeitung einzelner Teammitglieder, sowie der Einhaltung strikter Deadlines.

\subsection{Recherche und Setup}

Die Einarbeitung in die Basisthemen der Arbeit wurde gemeinsam im Team vorgenommen. Folgende Themen und deren Schwerpunkte wurden recherchiert. Im Anschluss wurde mittels Node ein lokaler Server f\"ur Test- und Pr\"asentationszwecke erstellt.

\begin{itemize}
\item \textbf{JavaScript, Node und npm} - verwendete Programmiersprache, serverseitiges Framework und dazugeh\"orirger Paketverwaltung.
\item \textbf{Schema.org und RDFa} - verwendetes Vokabular und Annotation der Daten.
\end{itemize}


\subsection{Implementation und Tests}

Aufgrund der Ergebnisse der Recherche wurde das Erstellen und Testen des Programmcodes unter den Teammitgliedern aufgeteilt. Zeitgleich erfolgte die Dokumentation der beiden Module, mittels Kommentaren im Programmcode und einer "Readme" Datei. 

\begin{itemize}
\item \textbf{Modul 1: RDFa Parser} \\
Mittels dem npm Modul "cheerio" wird auf Daten eines vorliegenden XHTML Dokuments zugegriffen. Im Anschluss werden mittels der RDFa Core Sequenz und dem npm Modul "rdf", RDF Triple erzeugt \cite{cheerioModule}, \cite{rdfModule}. 

\item \textbf{Modul 2: Webcrawler} \\
Mittels dem npm Modul "simplecrawler" wird, ausgehend von einer vorgegebenen URL und den Benutzereinstellungen, eine Liste von zusammenh\"angenden URLs erstellt. Dabei wird nach jedem Zugriff auf eine URL Modul 1 aufgerufen. Zu den Benutzereinstellungen z\"ahlt das "Black-/Whitelisten", daher die Beschr\"ankung der URL Dom\"anen, sowie die Suchtiefe, L\"ange der Verlinkung ausgehend von der Start URL \cite{simplecrawlerModule}.
\end{itemize}

\subsection{Zusammenf\"uhrung und Publikation}

Im Anschluss wurden die einzelnen Module zusammengef\"uhrt und nochmals anhand des "RDFa Test Suite Manifest" getestet, bevor das finale Programm als npm Modul ver\"offentlicht wurde \cite{rdfaTest}.


%------------------------------------------------

\section{Ergebnis}

Die zusammengef\"uhrten Module wurden als einzelnes Paket \"uber npm ver\"offentlicht \cite{rdfaCrawler}. Das Paket bietet zwei Methoden, "parseRDFa" und "crawler" an, um RDFa Inhalte aus dem "World Wide Web" zu extrahieren. \\

\textbf{parseRdfa(html, base)} \\
- mit R\"uckgabe der RDFa Triples in einer Liste.
\begin{itemize}
\item \textit{html (String)} - das HTML Dokument, welches RDFa Informationen beinhaltet.
\item \textit{base (String)} - die URL, die dem HTML Dokument zugrunde liegt.
\end{itemize}

\textbf{crawler(start, depth, callback, whitelist, blacklist)} \\
- ohne R\"uckgabewert.
\begin{itemize}
\item \textit{start (String)} - die URL, von der aus die Funktion startet.
\item \textit{depth (int)} - Tiefe der URL Links, die durchsucht werden sollen.
\item \textit{callback (Function)} - Funktion, die die gefundenen HTML Dokumente bearbeitet (RDFa Triple extrahieren und weiterverarbeiten).
\item \textit{whitelist (String Array)} - Optionale Liste mit Subdomains, die durchsucht werden sollen.
\item \textit{blacklist (String Array)} - Optionale Liste mit Subdomains, die niht durchsucht werden sollen.
\end{itemize}

\subsection{Beispielanwendung}

Der folgende Code implementiert beispielhaft die Verwendung des erstellten Moduls in Abh\"angigkeit des "Request" npm Moduls \cite{requestModule}. \\

\textit{npm Packet installieren:}
\begin{lstlisting}
npm install rdfa-parser
\end{lstlisting}

\textit{Parsen eines einzelnen HTML Dokuments:}
\begin{lstlisting}[language=JavaScript]
var rdfaParser = require("rdfa-parser");
var request = require("request");
let base = "http://booking.com";

request(base, function (error, response, html) {
    let triples = rdfaParser.parseRDFa(html, base);
    for (let i = 0; i < triples.length; i++) {
        console.log(triples[i].toString());
    }
});
\end{lstlisting}

\textit{Parsen mehrerer HTML Dokumente:}
\begin{lstlisting}[language=JavaScript]
var rdfaParser = require("rdfa-parser");
var request = require("request");
let start = "http://booking.com";
let depth = 2;
 
rdfaParser.crawler(start, depth, function (base) {
    request(base, function (error, response, html) {
        let triples = rdfaParser.parseRDFa(html, base);
        for (let i = 0; i < triples.length; i++) {
            console.log(triples[i].toString());
        }
    });
});
\end{lstlisting}


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{Referenzen}

\newpage
\bibliography{sample}	% use sample.bib from same absolute path (location)

\bibliographystyle{unsrt}

%----------------------------------------------------------------------------------------

\end{document}